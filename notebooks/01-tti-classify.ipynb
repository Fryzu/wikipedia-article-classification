{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd0f7083fe511133daffeb49be461125359fe7d6b3c2688ba4e67215bcacd5aa8a4",
   "display_name": "Python 3.7.7 64-bit ('CI': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../TTI/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "source": [
    "# Taksonomia, identyfikacja tekstu\n",
    "\n",
    "Dany jest fragment hierarchii klasyfkacji tematycznej z Wikipedii (https://en.wikipedia.org/wiki/Category:Main_topic_classifications) w postaci pliku CSV.\n",
    "Klasyfkacja jest grafem spójnym, gdzie węzły są tematami, a krawędzie reprezentują uszczegółowienie tematu.\n",
    "\n",
    "Celem projektu jest zapropnowanie i przetestowanie mechanizmu automatycznej klasyfikacji tekstu Wejściem jest plik tekstowy w języku angielskim. Wyjściem jest zbiór węzłów w/w klasyfikacji tematycznej.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dane wejściowe\n",
    "\n",
    "Dane wejściowe do zadania do graf spójny o 225765 węzłach, kady węzeł reprezentuje jedną kategorię. Graf nie jest uporządkowanym drzewem, może również zawierać pętle."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from TTI.CategoriesGraph import CategoriesGraph\n",
    "\n",
    "categories = CategoriesGraph()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading topics graph\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ilość krawędzi (339250, 2)\nIlość węzłów 225765\n"
     ]
    }
   ],
   "source": [
    "print(\"Ilość krawędzi\", categories._edge_list.shape)\n",
    "print(\"Ilość węzłów\", categories._graph.number_of_nodes())"
   ]
  },
  {
   "source": [
    "## Zbiór treningowy\n",
    "\n",
    "Zbiór treningowy został przygotowany z wykorzystaniem notebooka `01-tti-training-set-generate.ipynb`. Tam jest też więcej informacji o procesie generacji."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTI.config import DATABASE_PATH\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "table_name = \"training_set_25\"\n",
    "connection = sqlite3.connect(DATABASE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_sql('select * from {}'.format(table_name), connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Representation\"] = dataset[\"Representation\"].apply(lambda i : json.loads(i))\n",
    "dataset[\"Category\"] = dataset[\"Category\"].apply(lambda i : i[9:])\n",
    "dataset[\"Words\"] = dataset[\"Words\"].apply(lambda i : json.loads(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset size: (225765, 3)\nNumeric represntation vector size: 300\nNumber of nodes in the graph: 25\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", dataset.shape)\n",
    "print(\"Numeric represntation vector size:\", len(dataset.iloc[12][\"Representation\"]))\n",
    "print(\"Number of nodes in the graph:\", len(dataset.iloc[12][\"Words\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                Category  \\\n",
       "0             Main_topic_classifications   \n",
       "1                    Main topic articles   \n",
       "2                   Academic disciplines   \n",
       "3       Subfields by academic discipline   \n",
       "4                   Scholars by subfield   \n",
       "...                                  ...   \n",
       "225760              World Wide Web stubs   \n",
       "225761        Internet publication stubs   \n",
       "225762                     Website stubs   \n",
       "225763        Wikimedia Foundation stubs   \n",
       "225764                          Universe   \n",
       "\n",
       "                                                    Words  \\\n",
       "0       [academic, culture, human, entertainment, heal...   \n",
       "1       [academic, culture, human, entertainment, heal...   \n",
       "2       [academic, art, academics, euthenics, studies,...   \n",
       "3       [subfield, academic, areas, evolutionary, fiel...   \n",
       "4       [subfield, academic, architects, studies, clas...   \n",
       "...                                                   ...   \n",
       "225760  [internet, wide, system, technology, bioinform...   \n",
       "225761  [service, wide, entertainment, online, news, s...   \n",
       "225762  [websites, service, wide, entertainment, onlin...   \n",
       "225763  [websites, service, wide, entertainment, onlin...   \n",
       "225764  [academic, matter, culture, entertainment, hea...   \n",
       "\n",
       "                                           Representation  \n",
       "0       [-0.3755445182323456, 0.010519789531826973, -0...  \n",
       "1       [-0.40671899914741516, 0.013835961930453777, -...  \n",
       "2       [-0.09239675104618073, -0.46590009331703186, -...  \n",
       "3       [0.085173599421978, 0.010392077267169952, -0.3...  \n",
       "4       [-0.15292514860630035, -0.5975006222724915, -0...  \n",
       "...                                                   ...  \n",
       "225760  [0.216136172413826, -0.024581177160143852, -0....  \n",
       "225761  [0.2748589515686035, 0.2310565859079361, -0.34...  \n",
       "225762  [0.1632257103919983, 0.16291794180870056, -0.2...  \n",
       "225763  [0.19932252168655396, 0.19686073064804077, -0....  \n",
       "225764  [-0.24487195909023285, -0.2658485770225525, -0...  \n",
       "\n",
       "[225765 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Words</th>\n      <th>Representation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Main_topic_classifications</td>\n      <td>[academic, culture, human, entertainment, heal...</td>\n      <td>[-0.3755445182323456, 0.010519789531826973, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Main topic articles</td>\n      <td>[academic, culture, human, entertainment, heal...</td>\n      <td>[-0.40671899914741516, 0.013835961930453777, -...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Academic disciplines</td>\n      <td>[academic, art, academics, euthenics, studies,...</td>\n      <td>[-0.09239675104618073, -0.46590009331703186, -...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Subfields by academic discipline</td>\n      <td>[subfield, academic, areas, evolutionary, fiel...</td>\n      <td>[0.085173599421978, 0.010392077267169952, -0.3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Scholars by subfield</td>\n      <td>[subfield, academic, architects, studies, clas...</td>\n      <td>[-0.15292514860630035, -0.5975006222724915, -0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>225760</th>\n      <td>World Wide Web stubs</td>\n      <td>[internet, wide, system, technology, bioinform...</td>\n      <td>[0.216136172413826, -0.024581177160143852, -0....</td>\n    </tr>\n    <tr>\n      <th>225761</th>\n      <td>Internet publication stubs</td>\n      <td>[service, wide, entertainment, online, news, s...</td>\n      <td>[0.2748589515686035, 0.2310565859079361, -0.34...</td>\n    </tr>\n    <tr>\n      <th>225762</th>\n      <td>Website stubs</td>\n      <td>[websites, service, wide, entertainment, onlin...</td>\n      <td>[0.1632257103919983, 0.16291794180870056, -0.2...</td>\n    </tr>\n    <tr>\n      <th>225763</th>\n      <td>Wikimedia Foundation stubs</td>\n      <td>[websites, service, wide, entertainment, onlin...</td>\n      <td>[0.19932252168655396, 0.19686073064804077, -0....</td>\n    </tr>\n    <tr>\n      <th>225764</th>\n      <td>Universe</td>\n      <td>[academic, matter, culture, entertainment, hea...</td>\n      <td>[-0.24487195909023285, -0.2658485770225525, -0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>225765 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "source": [
    "## Wyszukiwanie najbardziej podobnych wektorów"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Do klasyfikacji posłuże się obliczaniem odległości geometrycznej pomiędzy wektorami reprezentacji doc2vec. Wektory o najmniejszej odległości zostaną zakwalifikowane jako najbardziej podobne."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         Category  \\\n",
       "2692  Machine learning algorithms   \n",
       "\n",
       "                                                  Words  \\\n",
       "2692  [checksum, algorithmic, trading, compression, ...   \n",
       "\n",
       "                                         Representation  \n",
       "2692  [0.302137166261673, 0.3030090630054474, -1.029...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n      <th>Words</th>\n      <th>Representation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2692</th>\n      <td>Machine learning algorithms</td>\n      <td>[checksum, algorithmic, trading, compression, ...</td>\n      <td>[0.302137166261673, 0.3030090630054474, -1.029...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "dataset.loc[dataset['Category'] == \"Machine learning algorithms\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Name of category Machine learning algorithms\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "name = dataset[\"Category\"][2692]\n",
    "vector = dataset[\"Representation\"][2692]\n",
    "\n",
    "print(\"Name of category\", name)"
   ]
  },
  {
   "source": [
    "Teraz należe znaleźć najbardziej podobne kategorie. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def find_simmilar(vector, count, df):\n",
    "    \"\"\" Finds 'count' best matching categories with vectors simmilar to 'vector'\"\"\"\n",
    "    categories = []\n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        vec = row[\"Representation\"]\n",
    "        name = row[\"Category\"]\n",
    "        categories.append((name, spatial.distance.cosine(vector, vec)))\n",
    "    sorted_categories = sorted(categories, key=lambda i: i[1])\n",
    "    return sorted_categories[0:count]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 225765/225765 [00:38<00:00, 5924.71it/s]\n"
     ]
    }
   ],
   "source": [
    "best_matching = find_simmilar(vector, 10, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Machine learning algorithms', 0.0),\n",
       " ('Heuristic algorithms', 0.03048611901635534),\n",
       " ('Cryptographic algorithms', 0.033841232971325574),\n",
       " ('Computer arithmetic algorithms', 0.034831473629752474),\n",
       " ('Data mining algorithms', 0.03487279219354722),\n",
       " ('Compression algorithms', 0.035516547716642144),\n",
       " ('Digit-by-digit algorithms', 0.03591962687260464),\n",
       " ('Algorithms', 0.03608325172873006),\n",
       " ('Bioinformatics algorithms', 0.03669657755224942),\n",
       " ('Approximation algorithms', 0.03737963968331992)]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "best_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('Heuristic algorithms', 0.03048611901635534)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "best_matching[1]"
   ]
  },
  {
   "source": [
    "# -----------------------"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcases_count, _ = dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:10000]\n",
    "Y = Y[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=1000, random_state=0).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "X[2345].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Category:Computational fields of study\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-1.05711957e-03 -1.12174821e-04 -6.70599227e-04  2.00716982e-04\n -1.12532716e-05 -1.19818910e-03 -1.00701256e-03  1.51766115e-03\n  1.38851278e-03  1.20256539e-03 -1.02999003e-03  1.59012235e-03\n -5.12395869e-04  1.04147161e-03 -1.88910679e-04 -1.18044345e-03\n  3.29556147e-04  3.28550494e-04  7.17254938e-04  1.46737450e-03\n  8.61251028e-04 -5.69134543e-04  1.21984247e-03 -7.90496066e-04\n -2.82717112e-04 -1.40814681e-03  3.09491617e-04 -4.13069793e-04\n -1.36248779e-03 -3.97055119e-05 -2.56790023e-04  1.99193833e-04\n  4.57675749e-04  1.64242432e-04 -1.70118772e-04  8.39911751e-04\n  1.49876974e-03 -1.35887484e-03 -1.46193802e-03 -1.29698543e-03\n  1.13061490e-03  4.60446638e-04 -8.44697934e-04 -4.18586103e-04\n  3.92560614e-04 -1.60691852e-03  1.19609770e-03 -1.41348923e-03\n  1.08640420e-03 -8.45222457e-05 -1.46952469e-03  1.60608278e-03\n -4.27623512e-04  1.15115463e-03  2.16798435e-05  1.13619689e-03\n  3.14626013e-05  1.47080002e-03 -1.65881461e-03 -1.47042359e-04\n -6.80239114e-04 -1.95611901e-05  4.85829427e-04 -5.07019344e-04\n  3.00552725e-04  9.96913295e-04 -1.43549743e-03 -1.75481749e-04\n -9.06471279e-04 -8.44579830e-04  5.84428723e-04  4.92196297e-04\n -2.56181112e-04 -1.59404858e-03  5.15849213e-04 -1.25946326e-03\n -5.85409929e-04 -5.06524579e-04 -1.37991458e-03  1.02065981e-03\n -1.20373932e-03  8.06969183e-04 -1.45958294e-03  5.13384468e-04\n  1.95905901e-04 -6.63767976e-04  1.54347601e-03 -8.10385565e-04\n  3.19290062e-04  7.88671896e-05  1.47709600e-03 -6.13130338e-04\n  3.46637040e-04  9.20038205e-04 -7.93774205e-04  1.36998121e-03\n -5.28734527e-04  5.67752169e-04 -1.36192038e-03 -1.29018468e-03\n -8.35585408e-04 -8.33809827e-05  1.47322111e-03 -4.50807769e-04\n  9.88051877e-04 -4.07564512e-04 -1.88141377e-04  5.08922036e-04\n  1.16993244e-04  1.31190900e-04 -8.29470751e-04  4.89950820e-04\n -3.31937481e-04  1.48364645e-03  2.27783501e-04  1.62656419e-03\n  6.20771374e-04  8.62140732e-04  1.27514813e-03  8.78580322e-04\n -2.34614257e-04  7.03568396e-04  1.38684700e-03  2.84489885e-04\n  1.56361493e-03 -9.04207816e-04  1.19785941e-03  1.00727775e-03\n  6.16470934e-04  9.29585600e-04  8.16004031e-05  1.34554191e-03\n  1.27612881e-03  3.68235080e-04  1.55027490e-03  1.57834333e-03\n  8.32954829e-04 -1.12463476e-05 -1.57821027e-03  3.55712225e-04\n  5.90891985e-04  7.37736467e-04 -1.48874952e-03  1.29009539e-03\n  3.85495689e-04  1.42908981e-03 -1.42835116e-03 -1.08473236e-03\n -4.23914869e-04  1.56111224e-03 -9.88631044e-04  2.75499944e-04\n -9.33356932e-04  1.58614735e-03 -1.27871661e-03  1.42677710e-03\n -2.57479929e-04 -2.09852413e-04 -1.68683080e-04  4.53274231e-04\n -2.96063721e-04  1.46724808e-03  5.58703905e-04 -1.85719682e-06\n -1.44393474e-04 -1.14537973e-03 -4.20980097e-04 -1.24457479e-03\n  4.40137403e-04 -1.21359399e-03 -1.38612522e-03 -1.40716357e-03\n  9.03638953e-04  1.58693537e-03 -1.17299310e-03 -1.04041887e-03\n -2.41254500e-04  1.37853238e-03 -9.78097669e-04  5.44907671e-05\n  2.90383352e-04  8.94300989e-04 -1.56898377e-03 -5.01372910e-04\n -5.47632226e-04 -4.93048923e-04  7.08205567e-04 -4.68627986e-04\n -1.69154766e-04 -7.09208834e-04 -6.77622971e-04 -1.52228065e-04\n -1.58828567e-03 -6.60329883e-04  1.11219485e-03  1.20913342e-03\n  1.45616184e-03  1.14660048e-04 -4.12935595e-04 -1.32893608e-03\n  1.05107611e-03 -9.36593337e-04 -1.06567240e-04  2.92538607e-05\n -1.04429456e-03  5.49213903e-04  1.53073168e-03  1.50990603e-03\n  1.02154876e-03  8.81676620e-04  9.74228722e-04 -1.49112742e-03\n  4.35472874e-04  1.61600835e-03  6.56703429e-04 -9.78550524e-04\n  1.24469487e-04  1.10312074e-03 -7.79370657e-06 -1.09899475e-03\n  5.54834143e-04  1.57351594e-03  1.00632908e-03  1.23282941e-03\n  6.04544766e-04  1.48530846e-04  1.64782943e-03  1.38901023e-03\n  1.65508327e-03  6.18561462e-04 -1.88564172e-05 -3.21197032e-04\n  1.34382094e-03  6.69610570e-04 -1.48776360e-03  1.17876414e-04\n -5.90742420e-05  1.44980068e-03 -3.00348678e-04 -6.79177989e-04\n  9.89316846e-04 -1.26218249e-03 -4.98922716e-04  7.82176794e-04\n  5.35731961e-04  1.40861818e-03  6.71846501e-05  9.16450401e-04\n -3.83602368e-04 -7.97643675e-04  6.38264231e-04 -3.91801266e-04\n -1.30973803e-03 -5.10019483e-04 -5.94273617e-04 -1.59030559e-03\n -1.48945523e-03 -1.14943890e-03  9.70767229e-04 -2.36914973e-04\n -5.97302569e-04  6.70650916e-04  1.15786667e-03  1.29027443e-03\n  1.26167681e-04  3.16318707e-04  1.29502523e-03  7.64396391e-04\n  3.51602561e-04 -1.48247927e-03  8.66172952e-04  1.15204370e-03\n -3.92968417e-04 -1.46732887e-03  1.63210416e-03  1.12474244e-03\n -1.53865200e-03 -5.42922120e-04 -1.44492066e-03 -1.91790052e-04\n -9.91406967e-04  1.59012794e-03 -1.02278893e-03 -9.17129917e-04\n -1.24915881e-04  9.13345022e-04  1.60187774e-03  1.38059608e-03\n  2.55861174e-04 -1.17688009e-03 -1.56134414e-03 -2.89360178e-04\n  1.03279937e-03 -6.60107704e-04 -1.60248950e-03  1.54751528e-03\n  7.74865795e-04 -1.50330117e-04  1.29592998e-04  2.58135842e-04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-23e5399ef73e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Category\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2345\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2345\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, sample_weight)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m         \u001b[0mx_squared_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_test_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    858\u001b[0m         X = self._validate_data(X, accept_sparse='csr', reset=False,\n\u001b[1;32m    859\u001b[0m                                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m                                 order='C', accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    861\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-1.05711957e-03 -1.12174821e-04 -6.70599227e-04  2.00716982e-04\n -1.12532716e-05 -1.19818910e-03 -1.00701256e-03  1.51766115e-03\n  1.38851278e-03  1.20256539e-03 -1.02999003e-03  1.59012235e-03\n -5.12395869e-04  1.04147161e-03 -1.88910679e-04 -1.18044345e-03\n  3.29556147e-04  3.28550494e-04  7.17254938e-04  1.46737450e-03\n  8.61251028e-04 -5.69134543e-04  1.21984247e-03 -7.90496066e-04\n -2.82717112e-04 -1.40814681e-03  3.09491617e-04 -4.13069793e-04\n -1.36248779e-03 -3.97055119e-05 -2.56790023e-04  1.99193833e-04\n  4.57675749e-04  1.64242432e-04 -1.70118772e-04  8.39911751e-04\n  1.49876974e-03 -1.35887484e-03 -1.46193802e-03 -1.29698543e-03\n  1.13061490e-03  4.60446638e-04 -8.44697934e-04 -4.18586103e-04\n  3.92560614e-04 -1.60691852e-03  1.19609770e-03 -1.41348923e-03\n  1.08640420e-03 -8.45222457e-05 -1.46952469e-03  1.60608278e-03\n -4.27623512e-04  1.15115463e-03  2.16798435e-05  1.13619689e-03\n  3.14626013e-05  1.47080002e-03 -1.65881461e-03 -1.47042359e-04\n -6.80239114e-04 -1.95611901e-05  4.85829427e-04 -5.07019344e-04\n  3.00552725e-04  9.96913295e-04 -1.43549743e-03 -1.75481749e-04\n -9.06471279e-04 -8.44579830e-04  5.84428723e-04  4.92196297e-04\n -2.56181112e-04 -1.59404858e-03  5.15849213e-04 -1.25946326e-03\n -5.85409929e-04 -5.06524579e-04 -1.37991458e-03  1.02065981e-03\n -1.20373932e-03  8.06969183e-04 -1.45958294e-03  5.13384468e-04\n  1.95905901e-04 -6.63767976e-04  1.54347601e-03 -8.10385565e-04\n  3.19290062e-04  7.88671896e-05  1.47709600e-03 -6.13130338e-04\n  3.46637040e-04  9.20038205e-04 -7.93774205e-04  1.36998121e-03\n -5.28734527e-04  5.67752169e-04 -1.36192038e-03 -1.29018468e-03\n -8.35585408e-04 -8.33809827e-05  1.47322111e-03 -4.50807769e-04\n  9.88051877e-04 -4.07564512e-04 -1.88141377e-04  5.08922036e-04\n  1.16993244e-04  1.31190900e-04 -8.29470751e-04  4.89950820e-04\n -3.31937481e-04  1.48364645e-03  2.27783501e-04  1.62656419e-03\n  6.20771374e-04  8.62140732e-04  1.27514813e-03  8.78580322e-04\n -2.34614257e-04  7.03568396e-04  1.38684700e-03  2.84489885e-04\n  1.56361493e-03 -9.04207816e-04  1.19785941e-03  1.00727775e-03\n  6.16470934e-04  9.29585600e-04  8.16004031e-05  1.34554191e-03\n  1.27612881e-03  3.68235080e-04  1.55027490e-03  1.57834333e-03\n  8.32954829e-04 -1.12463476e-05 -1.57821027e-03  3.55712225e-04\n  5.90891985e-04  7.37736467e-04 -1.48874952e-03  1.29009539e-03\n  3.85495689e-04  1.42908981e-03 -1.42835116e-03 -1.08473236e-03\n -4.23914869e-04  1.56111224e-03 -9.88631044e-04  2.75499944e-04\n -9.33356932e-04  1.58614735e-03 -1.27871661e-03  1.42677710e-03\n -2.57479929e-04 -2.09852413e-04 -1.68683080e-04  4.53274231e-04\n -2.96063721e-04  1.46724808e-03  5.58703905e-04 -1.85719682e-06\n -1.44393474e-04 -1.14537973e-03 -4.20980097e-04 -1.24457479e-03\n  4.40137403e-04 -1.21359399e-03 -1.38612522e-03 -1.40716357e-03\n  9.03638953e-04  1.58693537e-03 -1.17299310e-03 -1.04041887e-03\n -2.41254500e-04  1.37853238e-03 -9.78097669e-04  5.44907671e-05\n  2.90383352e-04  8.94300989e-04 -1.56898377e-03 -5.01372910e-04\n -5.47632226e-04 -4.93048923e-04  7.08205567e-04 -4.68627986e-04\n -1.69154766e-04 -7.09208834e-04 -6.77622971e-04 -1.52228065e-04\n -1.58828567e-03 -6.60329883e-04  1.11219485e-03  1.20913342e-03\n  1.45616184e-03  1.14660048e-04 -4.12935595e-04 -1.32893608e-03\n  1.05107611e-03 -9.36593337e-04 -1.06567240e-04  2.92538607e-05\n -1.04429456e-03  5.49213903e-04  1.53073168e-03  1.50990603e-03\n  1.02154876e-03  8.81676620e-04  9.74228722e-04 -1.49112742e-03\n  4.35472874e-04  1.61600835e-03  6.56703429e-04 -9.78550524e-04\n  1.24469487e-04  1.10312074e-03 -7.79370657e-06 -1.09899475e-03\n  5.54834143e-04  1.57351594e-03  1.00632908e-03  1.23282941e-03\n  6.04544766e-04  1.48530846e-04  1.64782943e-03  1.38901023e-03\n  1.65508327e-03  6.18561462e-04 -1.88564172e-05 -3.21197032e-04\n  1.34382094e-03  6.69610570e-04 -1.48776360e-03  1.17876414e-04\n -5.90742420e-05  1.44980068e-03 -3.00348678e-04 -6.79177989e-04\n  9.89316846e-04 -1.26218249e-03 -4.98922716e-04  7.82176794e-04\n  5.35731961e-04  1.40861818e-03  6.71846501e-05  9.16450401e-04\n -3.83602368e-04 -7.97643675e-04  6.38264231e-04 -3.91801266e-04\n -1.30973803e-03 -5.10019483e-04 -5.94273617e-04 -1.59030559e-03\n -1.48945523e-03 -1.14943890e-03  9.70767229e-04 -2.36914973e-04\n -5.97302569e-04  6.70650916e-04  1.15786667e-03  1.29027443e-03\n  1.26167681e-04  3.16318707e-04  1.29502523e-03  7.64396391e-04\n  3.51602561e-04 -1.48247927e-03  8.66172952e-04  1.15204370e-03\n -3.92968417e-04 -1.46732887e-03  1.63210416e-03  1.12474244e-03\n -1.53865200e-03 -5.42922120e-04 -1.44492066e-03 -1.91790052e-04\n -9.91406967e-04  1.59012794e-03 -1.02278893e-03 -9.17129917e-04\n -1.24915881e-04  9.13345022e-04  1.60187774e-03  1.38059608e-03\n  2.55861174e-04 -1.17688009e-03 -1.56134414e-03 -2.89360178e-04\n  1.03279937e-03 -6.60107704e-04 -1.60248950e-03  1.54751528e-03\n  7.74865795e-04 -1.50330117e-04  1.29592998e-04  2.58135842e-04].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "print(dataset[\"Category\"][2345])\n",
    "print(kmeans.predict(X[2345][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasterCategoryMap = {}\n",
    "for index, value in enumerate(kmeans.labels_):\n",
    "    if value in clasterCategoryMap:\n",
    "        clasterCategoryMap[value].add(dataset[\"Category\"][index])\n",
    "    else:\n",
    "        clasterCategoryMap[value] = set([dataset[\"Category\"][index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Category:(ISC)²',\n",
       " 'Category:Abstract data types',\n",
       " 'Category:Action theorists',\n",
       " 'Category:Administrative terminology',\n",
       " 'Category:Aerospace engineering award winners',\n",
       " 'Category:Aerospace engineers',\n",
       " 'Category:Agent-based software',\n",
       " 'Category:Agroecology',\n",
       " 'Category:Aircraft designers',\n",
       " 'Category:Algorithmic information theory',\n",
       " 'Category:Alternative medicine publications',\n",
       " 'Category:American civil engineering contractors',\n",
       " 'Category:Animal fats',\n",
       " 'Category:Anthropological categories of peoples',\n",
       " 'Category:Archaeologists',\n",
       " 'Category:Archaeologists appearing on Time Team',\n",
       " 'Category:Archaeologists by century',\n",
       " 'Category:Art history',\n",
       " 'Category:Artificial intelligence researchers',\n",
       " 'Category:Asphalt lakes',\n",
       " 'Category:Atmospheric sciences',\n",
       " 'Category:Belgian historians of religion',\n",
       " 'Category:Belief revision',\n",
       " 'Category:Biblical authorship debates',\n",
       " 'Category:Binary arithmetic',\n",
       " 'Category:Biogeographic realms',\n",
       " 'Category:Biological anthropology',\n",
       " 'Category:Biological warfare',\n",
       " 'Category:Biomedical engineering',\n",
       " 'Category:Biosimulation software',\n",
       " 'Category:Bonds in foreign currencies',\n",
       " 'Category:Breccias',\n",
       " 'Category:CPU sockets',\n",
       " 'Category:Capital budgeting',\n",
       " 'Category:Cardiovascular researchers',\n",
       " 'Category:Cash flow',\n",
       " 'Category:Cell BE architecture',\n",
       " 'Category:Central processing unit',\n",
       " 'Category:Chatterbots',\n",
       " 'Category:Cheminformatics',\n",
       " 'Category:Chinese historians of religion',\n",
       " 'Category:Civil engineering contractors',\n",
       " 'Category:Classical scholars of the Institute for Advanced Study',\n",
       " 'Category:Climate by country',\n",
       " 'Category:Compiler optimizations',\n",
       " 'Category:Computer access control frameworks',\n",
       " 'Category:Computer graphics professionals',\n",
       " 'Category:Computer security organizations',\n",
       " 'Category:Computer specialists by technology',\n",
       " 'Category:Costs',\n",
       " 'Category:Currency',\n",
       " 'Category:Debit cards',\n",
       " 'Category:Debugging',\n",
       " 'Category:Democratic free schools',\n",
       " 'Category:Demoscene',\n",
       " 'Category:Destroyed spacecraft',\n",
       " 'Category:Digital audio',\n",
       " 'Category:Digital forensics',\n",
       " 'Category:Digital forensics people',\n",
       " 'Category:Diskless workstations',\n",
       " 'Category:Drainage canals',\n",
       " 'Category:DreamWorks people',\n",
       " 'Category:Dynamical systems theorists',\n",
       " 'Category:Eating behaviors of humans',\n",
       " 'Category:Ecological economics',\n",
       " 'Category:Ecological niche',\n",
       " 'Category:Ecology awards',\n",
       " 'Category:Electric and magnetic fields in matter',\n",
       " 'Category:Electrical engineering books',\n",
       " 'Category:Electronics engineers',\n",
       " 'Category:Engineering literature',\n",
       " 'Category:Entomological writers',\n",
       " 'Category:Ethical consumerism',\n",
       " 'Category:Eugenics',\n",
       " 'Category:Fellows of the American Institute of Aeronautics and Astronautics',\n",
       " 'Category:Fellows of the Association for the Advancement of Artificial Intelligence',\n",
       " 'Category:Fellows of the Econometric Society',\n",
       " 'Category:Fellows of the SSAISB',\n",
       " 'Category:Fictional criminologists',\n",
       " 'Category:Fire protection organizations',\n",
       " 'Category:First Amendment scholars',\n",
       " 'Category:Foreign direct investment',\n",
       " 'Category:Forensic phenomena',\n",
       " 'Category:Forensic psychology',\n",
       " 'Category:Former landforms',\n",
       " 'Category:Foucault scholars',\n",
       " 'Category:Fruit morphology',\n",
       " 'Category:Functional analysts',\n",
       " 'Category:Fungi by adaptation',\n",
       " 'Category:Fungi naturalized in North America',\n",
       " 'Category:Genetic engineering by country',\n",
       " 'Category:Genetics books',\n",
       " 'Category:Geoarchaeologists',\n",
       " 'Category:Geographers of medieval Islam',\n",
       " 'Category:Geographical databases',\n",
       " 'Category:Geometric algorithms',\n",
       " 'Category:Glossaries of botany',\n",
       " 'Category:Health activism',\n",
       " 'Category:Historians of armoured warfare',\n",
       " 'Category:Historians of espionage by nationality',\n",
       " 'Category:Historians of genocides',\n",
       " 'Category:Historical continents',\n",
       " 'Category:Historical geographic information systems',\n",
       " 'Category:History of China by topic',\n",
       " 'Category:History of Italy by topic',\n",
       " 'Category:History of botany',\n",
       " 'Category:History of genetics',\n",
       " 'Category:Human migration',\n",
       " 'Category:Hume scholars',\n",
       " 'Category:Hydrogeology software',\n",
       " 'Category:Hydrology and urban planning',\n",
       " 'Category:Hypothetical galaxies',\n",
       " 'Category:Hypothetical impact events',\n",
       " 'Category:Ichthyological writers',\n",
       " 'Category:Identification friend or foe',\n",
       " 'Category:Impact craters on planets',\n",
       " 'Category:Individual Bibles',\n",
       " 'Category:Induction heating',\n",
       " 'Category:Institution of Chemical Engineers',\n",
       " 'Category:Interbank networks',\n",
       " 'Category:International finance',\n",
       " 'Category:International finance institutions',\n",
       " 'Category:Iranian historians of religion',\n",
       " 'Category:Irish Hispanists',\n",
       " 'Category:Italian genealogists',\n",
       " 'Category:Jurists of religious law',\n",
       " 'Category:Lasers',\n",
       " 'Category:Laws of robotics',\n",
       " 'Category:Legal history by issue',\n",
       " 'Category:Life extension',\n",
       " 'Category:Linguistic history',\n",
       " 'Category:Linguistic maps',\n",
       " 'Category:Linguists of Formosan languages',\n",
       " 'Category:Linguists of Polynesian languages',\n",
       " 'Category:Linguists of South Bougainville languages',\n",
       " 'Category:Lists of minor planets',\n",
       " 'Category:Literary critics of Spanish',\n",
       " 'Category:Lithostratigraphy of Russia',\n",
       " 'Category:Logic programming researchers',\n",
       " 'Category:Main topic articles',\n",
       " 'Category:Main_topic_classifications',\n",
       " 'Category:Malingering',\n",
       " 'Category:Manufacturers of industrial automation',\n",
       " 'Category:Maps in art',\n",
       " 'Category:Masonry',\n",
       " 'Category:Medical associations by country',\n",
       " 'Category:Meteor showers',\n",
       " 'Category:Metropolitan areas',\n",
       " 'Category:Microbiology institutes',\n",
       " 'Category:Military historians',\n",
       " 'Category:Minerals by crystal system',\n",
       " 'Category:Music cognition',\n",
       " 'Category:Muslim scholars of Islamic jurisprudence',\n",
       " 'Category:Mycologists by nationality',\n",
       " 'Category:Natural gas fields',\n",
       " 'Category:Nature writers by field',\n",
       " 'Category:Naturopathy',\n",
       " 'Category:Negotiation scholars',\n",
       " 'Category:Neural engineering',\n",
       " 'Category:Night-blooming plants',\n",
       " 'Category:Nvidia people',\n",
       " 'Category:OSI model',\n",
       " 'Category:Oceanographic organizations',\n",
       " 'Category:Open-source artificial intelligence',\n",
       " 'Category:Organizational psychologists',\n",
       " 'Category:Oriel and Laing Professors of the Interpretation of Holy Scripture',\n",
       " 'Category:Orion Molecular Cloud Complex',\n",
       " 'Category:Osteology',\n",
       " 'Category:Outward investment',\n",
       " 'Category:Petrology',\n",
       " 'Category:Phonetic algorithms',\n",
       " 'Category:Plants used in Native American cuisine',\n",
       " 'Category:Political geographers',\n",
       " 'Category:Population geneticists',\n",
       " 'Category:Pre-Columbian cultural areas',\n",
       " 'Category:Prehistoric life sorted by taxa',\n",
       " 'Category:Presidents of the Institution of Civil Engineers',\n",
       " 'Category:Primatology',\n",
       " 'Category:Procedural generation',\n",
       " 'Category:Process termination functions',\n",
       " 'Category:Psychiatric classification systems',\n",
       " 'Category:Radiation',\n",
       " 'Category:Radio timelines',\n",
       " 'Category:Radiodurants',\n",
       " 'Category:Railway civil engineers',\n",
       " 'Category:Rainforests by region',\n",
       " 'Category:Rammed earth',\n",
       " 'Category:Rescue and protection robots',\n",
       " 'Category:Rheumatologists by nationality',\n",
       " 'Category:Romanian gerontologists',\n",
       " 'Category:Rural geography',\n",
       " 'Category:SMPTE standards',\n",
       " 'Category:Satellite television',\n",
       " 'Category:Schools of informatics',\n",
       " 'Category:Seawalls',\n",
       " 'Category:Seaweeds',\n",
       " 'Category:Semantic Web people',\n",
       " 'Category:Set-top box',\n",
       " 'Category:Settlement geography',\n",
       " 'Category:Shoe museums',\n",
       " 'Category:Siegfried Line',\n",
       " 'Category:Social geography',\n",
       " 'Category:Software anomalies',\n",
       " 'Category:Software engineering papers',\n",
       " 'Category:Software engineering publications',\n",
       " 'Category:Solar phenomena',\n",
       " 'Category:South African nutritionists',\n",
       " 'Category:Space science organizations',\n",
       " 'Category:Space technology',\n",
       " 'Category:Spacecraft by country',\n",
       " 'Category:Spanish historians of religion',\n",
       " 'Category:Spiritual evolution',\n",
       " 'Category:Steam locomotive fireboxes',\n",
       " 'Category:Stellar physics',\n",
       " 'Category:Stereophotogrammetry',\n",
       " 'Category:Straits',\n",
       " 'Category:Support vector machines',\n",
       " 'Category:Supranational banks',\n",
       " 'Category:Sustainable building',\n",
       " 'Category:Sustainable development',\n",
       " 'Category:Synchronization',\n",
       " 'Category:Synthetic biology',\n",
       " 'Category:Systems of animal taxonomy',\n",
       " 'Category:Systems scientists by nationality',\n",
       " 'Category:Tax',\n",
       " 'Category:Taxonomic lists (species)',\n",
       " 'Category:Television timelines',\n",
       " 'Category:Temasek Holdings',\n",
       " 'Category:Underwater diving physics',\n",
       " 'Category:Urban geography',\n",
       " 'Category:Video games with stereoscopic 3D graphics',\n",
       " 'Category:Vitalism',\n",
       " 'Category:Volcanoes by sea or ocean',\n",
       " 'Category:Volleyball maps',\n",
       " 'Category:Weirs',\n",
       " \"Category:Wikipedia articles incorporating text from the 20th edition of Gray's Anatomy (1918)\",\n",
       " 'Category:Wikipedia books on medicine',\n",
       " 'Category:Winners of the Stockholm Prize in Criminology',\n",
       " 'Category:Works about former countries',\n",
       " 'Category:Works about robotics',\n",
       " 'Category:Works about the Great Depression',\n",
       " 'Category:World maps',\n",
       " 'Category:X Window System people',\n",
       " 'Category:Zoological societies',\n",
       " 'Category:Zoologists with author abbreviations',\n",
       " 'Category:Zoology museums',\n",
       " 'Category:Zoomusicology'}"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "clasterCategoryMap[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['archaeologist', 'anthropologists', 'fictional', 'subfield', 'or', 'armenian', 'archaeology', 'appearing', 'sinhalese', 'field', 'antiquarians', 'nationality', 'women', 'indigenous', 'ethnic', 'arab', 'by', 'burgher', 'people', 'crimean', 'igbo', 'national', 'catalan', 'tamil', 'of', 'flemish', 'occupations', 'archaeologists', 'australian', 'american', 'history', 'team', 'scientists', 'ethnicity', 'time', 'geoarchaeologists', 'origin', 'on', 'in', 'iranian', 'academics', 'basque', 'tatar', 'black', 'social', 'lists', 'stubs', 'research', 'british', 'century']\nCategory:Archaeologists by ethnicity\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"Words\"][10])\n",
    "print(dataset[\"Category\"][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_y = dataset[\"Category\"][:10000]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_y)\n",
    "Y = to_categorical((label_encoder.transform(train_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.array(xi) for xi in dataset[\"Representation\"][:10000]])\n",
    "numeric_vector_size = len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_10\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_30 (Dense)             (None, 256)               77056     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_31 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_32 (Dense)             (None, 256)               65792     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_33 (Dense)             (None, 10000)             2570000   \n=================================================================\nTotal params: 2,778,640\nTrainable params: 2,778,640\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(numeric_vector_size,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(10000, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "40/40 [==============================] - 2s 38ms/step - loss: 9.2134 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 9.2111 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 4/200\n",
      "40/40 [==============================] - 1s 37ms/step - loss: 9.2113 - accuracy: 0.0000e+00\n",
      "Epoch 5/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 6/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 1.2533e-04\n",
      "Epoch 7/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2114 - accuracy: 1.9259e-04\n",
      "Epoch 8/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2112 - accuracy: 3.2688e-05\n",
      "Epoch 9/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2113 - accuracy: 2.0574e-05\n",
      "Epoch 10/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2112 - accuracy: 5.8492e-05\n",
      "Epoch 11/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2112 - accuracy: 3.1486e-04\n",
      "Epoch 12/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 13/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2111 - accuracy: 0.0000e+00\n",
      "Epoch 14/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 3.1486e-04\n",
      "Epoch 15/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2113 - accuracy: 1.5119e-04\n",
      "Epoch 16/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 17/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 18/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2112 - accuracy: 2.6722e-04\n",
      "Epoch 19/200\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 9.2111 - accuracy: 1.1448e-04\n",
      "Epoch 20/200\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 21/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 0.0000e+00\n",
      "Epoch 22/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2112 - accuracy: 0.0000e+00\n",
      "Epoch 23/200\n",
      "40/40 [==============================] - 2s 50ms/step - loss: 9.2112 - accuracy: 1.2241e-04\n",
      "Epoch 24/200\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 9.2112 - accuracy: 2.6438e-05\n",
      "Epoch 25/200\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 9.2113 - accuracy: 3.5973e-05\n",
      "Epoch 26/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 4.2904e-05\n",
      "Epoch 27/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2112 - accuracy: 1.2403e-05\n",
      "Epoch 28/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2112 - accuracy: 3.1486e-04\n",
      "Epoch 29/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 3.9376e-05\n",
      "Epoch 30/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2112 - accuracy: 2.3461e-05\n",
      "Epoch 31/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 3.1486e-04\n",
      "Epoch 32/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2112 - accuracy: 8.8035e-05\n",
      "Epoch 33/200\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 9.2114 - accuracy: 2.0574e-05\n",
      "Epoch 34/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2112 - accuracy: 6.7359e-05\n",
      "Epoch 35/200\n",
      "40/40 [==============================] - 2s 45ms/step - loss: 9.2113 - accuracy: 1.4091e-04\n",
      "Epoch 36/200\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 9.2114 - accuracy: 8.2430e-05\n",
      "Epoch 37/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2111 - accuracy: 2.6438e-05\n",
      "Epoch 38/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2110 - accuracy: 1.1448e-04\n",
      "Epoch 39/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2112 - accuracy: 1.5119e-04\n",
      "Epoch 40/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2113 - accuracy: 1.4060e-04\n",
      "Epoch 41/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2112 - accuracy: 1.3108e-04\n",
      "Epoch 42/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2112 - accuracy: 1.2403e-05\n",
      "Epoch 43/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2113 - accuracy: 7.2123e-05\n",
      "Epoch 44/200\n",
      "40/40 [==============================] - 2s 43ms/step - loss: 9.2112 - accuracy: 2.3461e-05\n",
      "Epoch 45/200\n",
      "40/40 [==============================] - 2s 41ms/step - loss: 9.2112 - accuracy: 1.5119e-04\n",
      "Epoch 46/200\n",
      "40/40 [==============================] - 2s 40ms/step - loss: 9.2112 - accuracy: 1.5119e-04\n",
      "Epoch 47/200\n",
      "40/40 [==============================] - 2s 44ms/step - loss: 9.2113 - accuracy: 0.0000e+00\n",
      "Epoch 48/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2114 - accuracy: 1.3108e-04\n",
      "Epoch 49/200\n",
      "40/40 [==============================] - 2s 42ms/step - loss: 9.2111 - accuracy: 2.3546e-04\n",
      "Epoch 50/200\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 9.2112 - accuracy: 1.0034e-04\n",
      "Epoch 51/200\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 9.2113 - accuracy: 4.1013e-04\n",
      "Epoch 52/200\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 9.2112 - accuracy: 4.1013e-04\n",
      "Epoch 53/200\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 9.2112 - accuracy: 3.1486e-04\n",
      "Epoch 54/200\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 9.2112 - accuracy: 1.1448e-04\n",
      "Epoch 55/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2113 - accuracy: 1.2241e-04\n",
      "Epoch 56/200\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 9.2112 - accuracy: 2.1164e-04\n",
      "Epoch 57/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2113 - accuracy: 1.2403e-05\n",
      "Epoch 58/200\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 9.2111 - accuracy: 1.5119e-04\n",
      "Epoch 59/200\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 9.2112 - accuracy: 3.5973e-05\n",
      "Epoch 60/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2113 - accuracy: 1.3108e-04\n",
      "Epoch 61/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2112 - accuracy: 1.7772e-05\n",
      "Epoch 62/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2113 - accuracy: 1.7671e-04\n",
      "Epoch 63/200\n",
      "40/40 [==============================] - 2s 51ms/step - loss: 9.2112 - accuracy: 3.5973e-05\n",
      "Epoch 64/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2113 - accuracy: 1.5050e-05\n",
      "Epoch 65/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2112 - accuracy: 1.0034e-04\n",
      "Epoch 66/200\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 9.2111 - accuracy: 9.3990e-05\n",
      "Epoch 67/200\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 9.2112 - accuracy: 3.1486e-04\n",
      "Epoch 68/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2113 - accuracy: 2.6722e-04\n",
      "Epoch 69/200\n",
      "40/40 [==============================] - 2s 46ms/step - loss: 9.2112 - accuracy: 3.2688e-05\n",
      "Epoch 70/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2111 - accuracy: 0.0000e+00\n",
      "Epoch 71/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2112 - accuracy: 3.1486e-04\n",
      "Epoch 72/200\n",
      "40/40 [==============================] - 2s 47ms/step - loss: 9.2114 - accuracy: 9.3990e-05\n",
      "Epoch 73/200\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 9.2113 - accuracy: 0.0000e+00\n",
      "Epoch 74/200\n",
      "40/40 [==============================] - 2s 48ms/step - loss: 9.2112 - accuracy: 1.5119e-04\n",
      "Epoch 75/200\n",
      "40/40 [==============================] - 2s 49ms/step - loss: 9.2114 - accuracy: 7.3210e-06\n",
      "Epoch 76/200\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 9.2114 - accuracy: 6.7359e-05\n",
      "Epoch 77/200\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 9.2113 - accuracy: 9.8282e-06\n",
      "Epoch 78/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2111 - accuracy: 2.6722e-04\n",
      "Epoch 79/200\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 9.2113 - accuracy: 5.8492e-05\n",
      "Epoch 80/200\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 9.2112 - accuracy: 2.9512e-05\n",
      "Epoch 81/200\n",
      "40/40 [==============================] - 2s 60ms/step - loss: 9.2112 - accuracy: 3.1486e-04\n",
      "Epoch 82/200\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 9.2112 - accuracy: 7.2123e-05\n",
      "Epoch 83/200\n",
      "40/40 [==============================] - 2s 58ms/step - loss: 9.2112 - accuracy: 9.8282e-06\n",
      "Epoch 84/200\n",
      " 2/40 [>.............................] - ETA: 2s - loss: 9.2099 - accuracy: 0.0000e+00"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-7362e96fb51d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/CI/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mstep_increment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep_increment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;34m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X, Y, epochs=200, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=200)\n",
    "# pca.fit(X)\n",
    "# print(sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# model.fit(X, Y)"
   ]
  },
  {
   "source": [
    "## Zapisywanie modelu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cPickle\n",
    "\n",
    "# # save the classifier\n",
    "# with open('models/knn.pkl', 'wb') as fid:\n",
    "#     cPickle.dump(model, fid)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}